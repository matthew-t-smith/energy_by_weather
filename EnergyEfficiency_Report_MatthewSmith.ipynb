{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Case Study in Energy Efficiency\n",
    "## Matthew T. Smith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "A household’s normal energy usage most largely fluctuates due to the heating and cooling of the space based on weather. This particular analysis looks to model and predict a single household’s usage as it relates to that historical weather data. This compact model can be passed between an energy supplier or retailer and the consumer without having to hand off a user’s actual usage trends. Businesses can rely on normal meter reads and predictions from such a model to price their utility, and the consumers can better see how they use their energy and budget or plan accordingly. This small example of “federated learning” keeps privacy at the forefront of the model’s architecture. Through the use of the Keras Deep Learning library in Python on top of Google’s TensorFlow’s backend, a linear regression model is trained on a 10-month electricity usage report for an apartment in Wellington, New Zealand paired with the weather (high/low temperature, relative humidity, and rainfall) from that same time period. Predictions are then made on sample weather data to demonstrate probable energy consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Software\n",
    "- **Keras Framework** [https://github.com/keras-team/keras]\n",
    "- **TensorFlow Framework** [https://www.tensorflow.org/install/]\n",
    "- **Python 3.7** [https://www.python.org/downloads/]\n",
    "- **h5py** [http://docs.h5py.org/en/latest/build.html]\n",
    "- **numpy** [https://scipy.org/install.html]\n",
    "- **pandas** [https://pandas.pydata.org/pandas-docs/stable/install.html]\n",
    "- **matplotlib** [https://scipy.org/install.html]\n",
    "\n",
    "#### Hardware\n",
    "- **Operating System:** macOS Mojave 10.14.4\n",
    "- **Processor:** 2.3 GHz Intel Core i7\n",
    "- **Memory:** 16 GB 1600 MHz DDR3\n",
    "- **GPU**: N/A"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "#### Weather\n",
    "- CliFlo – the web application providing access to New Zealand’s National Climate Database [https://cliflo.niwa.co.nz/]\n",
    "- Collected from Wellington, NZ’s Greta Point Weather Station\n",
    "    - Closest station to the home with complete data\n",
    "- 10-month dataset of high and low temperatures (Celsius), relative humidity (percent), and rainfall (millimeters)\n",
    "\n",
    "#### Energy Consumption\n",
    "- Powershop NZ – Energy retailer for the home [https://www.powershop.co.nz/]\n",
    "- Half-hourly consumption (kWh) fed from smart meter at the property\n",
    "- The data downloaded is from my own home in NZ; consumption data is not freely available via the retailer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Energy: \n",
      "\n",
      "         reading_start          reading_end  usage\n",
      "0  29/03/2018 00:00:01  29/03/2018 00:30:00   0.05\n",
      "1  29/03/2018 00:00:01  30/03/2018 00:00:00  11.51\n",
      "2  29/03/2018 00:30:01  29/03/2018 01:00:00   0.04\n",
      "\n",
      "Weather: \n",
      "\n",
      "                     Station         Date(UTC) Tmax(C) Period(Hrs)  Tmin(C)  \\\n",
      "Wellington   Greta Point Cws  29/03/2018 00:00    20.5           1     19.5   \n",
      "Wellington   Greta Point Cws  29/03/2018 01:00    22.4           1     19.9   \n",
      "Wellington   Greta Point Cws  29/03/2018 02:00    23.2           1     22.4   \n",
      "\n",
      "            Period(Hrs).1 Tgmin(C) Period(Hrs).2 Tmean(C) RHmean(%)  \\\n",
      "Wellington              1        -             -     19.8        66   \n",
      "Wellington              1        -             -     20.8        61   \n",
      "Wellington              1        -             -     22.8        51   \n",
      "\n",
      "           Period(Hrs).3 Freq  \n",
      "Wellington             1    H  \n",
      "Wellington             1    H  \n",
      "Wellington             1    H  \n",
      "\n",
      "Rainfall: \n",
      "\n",
      "                       Station         Date(UTC)  Amount(mm)  Period(Hrs) Freq\n",
      "0  Wellington, Greta Point Cws  29/03/2018 00:00         0.0            1    H\n",
      "1  Wellington, Greta Point Cws  29/03/2018 01:00         0.0            1    H\n",
      "2  Wellington, Greta Point Cws  29/03/2018 02:00         0.0            1    H\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ENERGY = './data/energy_data_raw.csv'\n",
    "WEATHER = './data/wellington_weather_raw.csv'\n",
    "RAIN = './data/wellington_rain_raw.csv'\n",
    "\n",
    "E = pd.read_csv(ENERGY)\n",
    "W = pd.read_csv(WEATHER)\n",
    "R = pd.read_csv(RAIN)\n",
    "\n",
    "print(\"\\nEnergy: \\n\")\n",
    "print(E.head(3))\n",
    "print(\"\\nWeather: \\n\")\n",
    "print(W.head(3))\n",
    "print(\"\\nRainfall: \\n\")\n",
    "print(R.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "#### Installation\n",
    "The project and data can be downloaded from here: [https://github.com/matthew-t-smith/energy_by_weather]\n",
    "\n",
    "From the home directory, run:\n",
    "\n",
    "`~/energy_by_weather $ pip3 install -r requirements.txt`\n",
    "\n",
    "#### Data Cleaning\n",
    "The raw data already exists in the `/data/` directory, as does the cleaning script and output data. To re-clean the data, from the data directory, run:\n",
    "\n",
    "`~/energy_by_weather/data $ python3 data_prep.py`\n",
    "\n",
    "#### Model Creation and Training\n",
    "The model will be created new, train, and save as an `.h5` file, as already exists in the directory currently. Plots will also be re-saved over the existing files when running. To re-create and re-train the model, from the home directory, run:\n",
    "\n",
    "`~/energy_by_weather $ python3 model.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "# Read half-hourly energy data and merge usages to hourly increments\n",
    "raw_data = pd.read_csv(ENERGY, skiprows=(lambda x: x == 49*(int(x/49)) + 2))\n",
    "reads = []\n",
    "for i in range(0, raw_data.shape[0] - 2, 2):\n",
    "    new_row = [pd.to_datetime(raw_data.at[i, 'reading_start'], dayfirst=True).round(\n",
    "        'H'), (raw_data.at[i, 'usage'] + raw_data.at[(i+1), 'usage'])]\n",
    "    reads.append(new_row)\n",
    "\n",
    "energy = pd.DataFrame(reads, columns=['Date(UTC)', 'usage'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The energy data is first loaded, using a `lambda` function to skip every 49th entry (an added 24-hour accumulative reading). The reading datetime and energy usage are then extracted by adding the first thirty-minute read of the hour to the second thirty-minute read of the hour (and rounding to the hour to negate the one-second offset). We must do this because our weather data is only hourly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeslice rain, weather and energy data\n",
    "def time_slice(dataset):\n",
    "    dataset['Date(UTC)'] = pd.to_datetime(dataset['Date(UTC)'], dayfirst=True)\n",
    "    dataset.set_index('Date(UTC)', inplace=True)\n",
    "    dataset = dataset['2018-03-29':'2019-01-29']\n",
    "    dataset.reset_index(inplace=True)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "rain = pd.read_csv(RAIN, usecols=['Date(UTC)', 'Amount(mm)'], dayfirst=True)\n",
    "rain = time_slice(rain)\n",
    "weather = pd.read_csv(\n",
    "    WEATHER, usecols=['Date(UTC)', 'Tmax(C)', 'Tmin(C)', 'RHmean(%)'], dayfirst=True)\n",
    "weather = time_slice(weather)\n",
    "energy = time_slice(energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rain, weather, and energy data are then read (with only the relevant columns) and clipped to the 10-month period, mostly as an assurance that what was downloaded from the raw sources is equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single DataFrame to compile feed-in data\n",
    "final = []\n",
    "for index, row in weather.iterrows():\n",
    "    energy_filter = energy['Date(UTC)'] == row['Date(UTC)']\n",
    "    rain_filter = rain['Date(UTC)'] == row['Date(UTC)']\n",
    "\n",
    "    rainfall = rain[rain_filter]['Amount(mm)'].values\n",
    "    try:\n",
    "        rainfall = float(rainfall)\n",
    "    except (TypeError, ValueError):\n",
    "        rainfall = -1\n",
    "\n",
    "    usage = energy[energy_filter]['usage'].values\n",
    "    try:\n",
    "        usage = float(usage)\n",
    "    except (TypeError, ValueError):\n",
    "        usage = -1\n",
    "\n",
    "    try:\n",
    "        row['RHmean(%)'] = float(row['RHmean(%)']) / 100\n",
    "    except (TypeError, ValueError):\n",
    "        row['RHmean(%)'] = -1\n",
    "\n",
    "    entry = [row['Date(UTC)'], row['Tmax(C)'], row['Tmin(C)'],\n",
    "             row['RHmean(%)'], rainfall, usage]\n",
    "    final.append(entry)\n",
    "\n",
    "data = pd.DataFrame(final, columns=[\n",
    "    'DateTime', 'Tmax(C)', 'Tmin(C)', 'RHmean(%)', 'Rain(mm)', 'Usage(kWh)'])\n",
    "data.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next iterate over the `weather` dataframe (since it contains the most information to reuse), pair up the hour timeslot in the corresponding `rainfall` and `energy` dataframes, and then accumulate the values into new rows for our `final` dataframe. If there are any errors with incomplete, non-numeric, or otherwise empty nodes, we flag them to be dropped later with `-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7356, 6)\n",
      "(7126, 6)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "drops = []\n",
    "for index, row in data.iterrows():\n",
    "    if (row.isin(['-', -1.0]).any() or (row['Usage(kWh)'] >= 10)):\n",
    "        drops.append(index)\n",
    "data.drop(drops, axis='index', inplace=True)\n",
    "print(data.shape)\n",
    "data.to_csv('./data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, but not least, we drop the relevant rows (230 in total). We see the final shape of our data brings us to 7126 valid rows to use as input!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Normalizing function for dataset\n",
    "def norm(d):\n",
    "    return (d - d.min()) / (d.max() - d.min())\n",
    "\n",
    "def denorm(y, d):\n",
    "    return (y * (d.max() - d.min()) + d.min())\n",
    "\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('./data/data.csv',\n",
    "                   usecols=['Tmax(C)', 'Tmin(C)', 'RHmean(%)', 'Rain(mm)', 'Usage(kWh)'])\n",
    "dataset = data.values.astype(float)\n",
    "X = dataset[:, 0:4]\n",
    "Y = dataset[:, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load and split our data up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the dataset\n",
    "normed_X = np.apply_along_axis(norm, axis=0, arr=X)\n",
    "normed_Y = norm(Y)\n",
    "\n",
    "# Split datasets into training and test\n",
    "split = 0.8\n",
    "val = math.floor(split*len(normed_X))\n",
    "train_X = normed_X[:val]\n",
    "train_Y = normed_Y[:val]\n",
    "test_X = normed_X[val:]\n",
    "test_Y = normed_Y[val:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then normalize each column to fit between 0 and 1, and then split that data 80-20 into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# Build the model with layers\n",
    "model = Sequential()\n",
    "model.add(Dense(4, input_dim=4, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1))\n",
    "adam = Adam(lr=1e-5)\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=adam, metrics=['mean_absolute_error', 'mean_squared_error'])\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(train_X,\n",
    "                    train_Y,\n",
    "                    epochs=200,\n",
    "                    validation_data=(test_X, test_Y),\n",
    "                    verbose=0)\n",
    "model.save('energy.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we define our model. We expect 4 weather input nodes, which we then pass to a `Dropout` layer to help with overfitting because our dataset is relatively small. We repeat this again before finally pushing into a single output node. We are minimizing the loss of our `mean_squared_error` because our dataset is continuous rather than categorical. We will see the trends of training over these 200 epochs in the plots to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Abs Error')\n",
    "plt.plot(hist['epoch'], hist['mean_absolute_error'],\n",
    "         label='Train Error')\n",
    "plt.plot(hist['epoch'], hist['val_mean_absolute_error'],\n",
    "         label='Val Error')\n",
    "plt.legend()\n",
    "plt.savefig('./plots/mae.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Square Error')\n",
    "plt.plot(hist['epoch'], hist['mean_squared_error'],\n",
    "         label='Train Error')\n",
    "plt.plot(hist['epoch'], hist['val_mean_squared_error'],\n",
    "         label='Val Error')\n",
    "plt.legend()\n",
    "plt.savefig('./plots/mse.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Usage (kWh)')\n",
    "plt.ylabel('Temp (C)')\n",
    "plt.plot(denorm(model.predict(test_X[-10:, :]),\n",
    "                Y), X[-10:, 0], label='High')\n",
    "plt.plot(denorm(model.predict(test_X[-10:, :]),\n",
    "                Y), X[-10:, 1], label='Low')\n",
    "plt.legend()\n",
    "plt.savefig('./plots/predictions.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mae](./plots/mae.png)\n",
    "![mse](./plots/mse.png)\n",
    "![predictions](./plots/predictions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the `mean_squared_error` decrease nicely over the first 50 or so epochs before leveling off near 0. In the final plot, we can see the inverse linearity we hoped for with the usage increasing where the temperature decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improvements\n",
    "\n",
    "- The model is training using minimal weather data (temperature, humidity and rainfall), but more points of data could be collected to improve predictions.\n",
    "- Other inputs could later include presence or absence in the house as energy consumption will be higher when individuals are home, minus a general baseline of appliances and electronics that run constantly.\n",
    "- The trained model for this home could now be safely passed between Powershop NZ and the consumer to continue training as time goes on. Powershop NZ would not be able to reverse-engineer the actual half-hourly data usage if it were fed to the user rather than the retailer (as is the case here for data collection).\n",
    "- Powershop could better estimate each home’s usage to better expect spikes in use and better price available electricity for future consumption.\n",
    "- Similarly, the consumer can use these predictions to see how they might use energy compared to a weather forecast in the coming week(s) or month(s) in order to budget accordingly.\n",
    "\n",
    "#### Looking Ahead\n",
    "\n",
    "- Energy efficiency is just one small set of data individuals carry that can benefit from a federated learning model; more privacy on the user’s end and less resources for a business to have to manage.\n",
    "- In an ideal world, corporations might collect a large set of models that are trained and personalized to each consumer rather than actual terabytes of private user data. These models could be deleted or restarted at a user’s request, but the corporations would still benefit from the personalized training done over time.\n",
    "- Advertising, services, recommendations, and other “smart” metrics can gradually improve over time not only for individual users, but for a userbase as a whole if the collective models are then trained with each other.\n",
    "- This type of learning becomes especially beneficial in high-sensitivity datasets like those found in healthcare. Diagnostic prediction models could benefit by training on large sets of cases without ever transporting that sensitive data outside of the hospital where it was collected in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YouTube Links\n",
    "- **2-Minute Version:** [https://youtu.be/287fuFMdb_4]\n",
    "- **15-Minute Version:** [https://youtu.be/iY6oBf72PRU]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
